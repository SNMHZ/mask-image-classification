{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize, ToTensor, Normalize\n",
    "import torchvision.models as models\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "built-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 데이터셋 폴더 경로를 지정해주세요.\n",
    "train_dir = '/opt/ml/input/data/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c650b7ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-organizer",
   "metadata": {},
   "source": [
    "## 1. Model 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20fdbb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingLoss(nn.Module):\n",
    "    def __init__(self, classes=3, smoothing=0.0, dim=-1):\n",
    "        super(LabelSmoothingLoss, self).__init__()\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.cls = classes\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred = pred.log_softmax(dim=self.dim)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (self.cls - 1))\n",
    "            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acknowledged-easter",
   "metadata": {},
   "outputs": [],
   "source": [
    "net_age = models.resnet152(pretrained=True)\n",
    "net_gender = models.resnet152(pretrained=True)\n",
    "net_mask = models.resnet152(pretrained=True)\n",
    "# net_age = models.resnet18(pretrained=True)\n",
    "# net_gender = models.resnet18(pretrained=True)\n",
    "# net_mask = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9d77e13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0400,  0.0018, -0.0406,  ...,  0.0374, -0.0032, -0.0401],\n",
       "        [ 0.0489, -0.0444,  0.0136,  ...,  0.0277,  0.0516,  0.0035],\n",
       "        [-0.0239,  0.0276,  0.0374,  ...,  0.0131,  0.0393,  0.0204]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net_age.fc = torch.nn.Linear(in_features=2048, out_features=100, bias=True)\n",
    "net_age.to(device)\n",
    "net_gender.fc = torch.nn.Linear(in_features=2048, out_features=2, bias=True)\n",
    "net_gender.to(device)\n",
    "net_mask.fc = torch.nn.Linear(in_features=2048, out_features=3, bias=True)\n",
    "net_mask.to(device)\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "loss_fn = LabelSmoothingLoss() \n",
    "optim_age = torch.optim.Adam(net_age.parameters(), lr=LEARNING_RATE)\n",
    "torch.nn.init.xavier_uniform_(net_age.fc.weight)\n",
    "optim_gender = torch.optim.Adam(net_gender.parameters(), lr=LEARNING_RATE)\n",
    "torch.nn.init.xavier_uniform_(net_gender.fc.weight)\n",
    "optim_mask = torch.optim.Adam(net_mask.parameters(), lr=LEARNING_RATE)\n",
    "torch.nn.init.xavier_uniform_(net_mask.fc.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 2. Train Dataset 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575fde9",
   "metadata": {},
   "source": [
    "#### age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAge(Dataset):\n",
    "    def __init__(self, train_path, transform):\n",
    "        img_path = os.path.join(train_path, 'images')\n",
    "        self.transform = transform\n",
    "        self.traininfo = pd.read_csv(os.path.join(train_path, 'train.csv'))\n",
    "        img_paths = img_path + '/' + self.traininfo['path']\n",
    "        file_names = []\n",
    "        for path in img_paths:\n",
    "            names = os.listdir(path)\n",
    "            names = [name for name in names if name[0] != '.']\n",
    "            file_names.append(names)\n",
    "\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        for i in range(len(self.traininfo['path'])):\n",
    "            for name in file_names[i]:\n",
    "                self.X.append(os.path.join(img_paths[i], name))\n",
    "                self.Y.append(int(self.traininfo['age'][i]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.X[index])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        target = self.Y[index]\n",
    "        return image, target\n",
    "\n",
    "    def _classify(self, age):\n",
    "        if age < 30 :\n",
    "            return 0\n",
    "        elif 30 <= age < 60:\n",
    "            return 1\n",
    "        else:\n",
    "            return 2    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d8f33",
   "metadata": {},
   "source": [
    "#### gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0914be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainGender(Dataset):\n",
    "    def __init__(self, train_path, transform):\n",
    "        img_path = os.path.join(train_path, 'images')\n",
    "        self.transform = transform\n",
    "        self.traininfo = pd.read_csv(os.path.join(train_path, 'train.csv'))\n",
    "        img_paths = img_path + '/' + self.traininfo['path']\n",
    "        file_names = []\n",
    "        for path in img_paths:\n",
    "            names = os.listdir(path)\n",
    "            names = [name for name in names if name[0] != '.']\n",
    "            file_names.append(names)\n",
    "\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        self.filter = {'male' : 0, 'female' : 1}\n",
    "        for i in range(len(self.traininfo['path'])):\n",
    "            for name in file_names[i]:\n",
    "                self.X.append(os.path.join(img_paths[i], name))\n",
    "                self.Y.append(self.filter[self.traininfo['gender'][i]])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.X[index])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        target = self.Y[index]\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932fa84",
   "metadata": {},
   "source": [
    "#### mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fac4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainMask(Dataset):\n",
    "    def __init__(self, train_path, transform):\n",
    "        img_path = os.path.join(train_path, 'images')\n",
    "        self.transform = transform\n",
    "        self.traininfo = pd.read_csv(os.path.join(train_path, 'train.csv'))\n",
    "        img_paths = img_path + '/' + self.traininfo['path']\n",
    "        file_names = []\n",
    "        for path in img_paths:\n",
    "            names = os.listdir(path)\n",
    "            names = [name for name in names if name[0] != '.']\n",
    "            file_names.append(names)\n",
    "\n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        for i in range(len(self.traininfo['path'])):\n",
    "            for name in file_names[i]:\n",
    "                self.X.append(os.path.join(img_paths[i], name))\n",
    "                self.Y.append(self._classify(name[0]))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.X[index])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        target = self.Y[index]\n",
    "        return image, target\n",
    "\n",
    "    def _classify(self, mask):\n",
    "        if mask == 'm':\n",
    "            return 0\n",
    "        elif mask == 'i':\n",
    "            return 1\n",
    "        else:\n",
    "            return 2\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f3dd386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[ 1.2647,  1.2647,  1.2647,  ...,  1.5000,  1.5000,  1.5000],\n",
       "          [ 1.2647,  1.2647,  1.2647,  ...,  1.5000,  1.5000,  1.5000],\n",
       "          [ 1.2647,  1.2647,  1.2647,  ...,  1.4804,  1.5000,  1.5000],\n",
       "          ...,\n",
       "          [-0.0098, -0.0294, -0.0294,  ...,  0.4608,  0.5588,  0.5980],\n",
       "          [-0.3431, -0.3627, -0.3627,  ...,  0.4804,  0.5588,  0.6176],\n",
       "          [-0.8137, -0.8137, -0.8137,  ...,  0.4804,  0.5784,  0.6176]],\n",
       " \n",
       "         [[ 1.2451,  1.2451,  1.2451,  ...,  1.4804,  1.4804,  1.4804],\n",
       "          [ 1.2451,  1.2451,  1.2451,  ...,  1.4804,  1.4804,  1.4804],\n",
       "          [ 1.2451,  1.2451,  1.2451,  ...,  1.4608,  1.4804,  1.4804],\n",
       "          ...,\n",
       "          [-0.9314, -0.9510, -0.9510,  ..., -0.5196, -0.4804, -0.4412],\n",
       "          [-1.2647, -1.2843, -1.2843,  ..., -0.5000, -0.4608, -0.4020],\n",
       "          [-1.7353, -1.7353, -1.7353,  ..., -0.5000, -0.4412, -0.4020]],\n",
       " \n",
       "         [[ 1.2059,  1.2059,  1.2059,  ...,  1.4412,  1.4412,  1.4412],\n",
       "          [ 1.2059,  1.2059,  1.2059,  ...,  1.4412,  1.4412,  1.4412],\n",
       "          [ 1.2059,  1.2059,  1.2059,  ...,  1.4216,  1.4412,  1.4412],\n",
       "          ...,\n",
       "          [-1.4804, -1.5000, -1.5000,  ..., -1.2059, -1.1471, -1.1078],\n",
       "          [-1.8137, -1.8333, -1.8333,  ..., -1.1863, -1.1863, -1.1275],\n",
       "          [-2.2843, -2.2843, -2.2843,  ..., -1.2255, -1.1667, -1.1275]]]),\n",
       " 45)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "age_train = TrainAge(train_dir,transform)\n",
    "gender_train = TrainGender(train_dir,transform)\n",
    "mask_train = TrainMask(train_dir,transform)\n",
    "age_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea0da0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCH = 3\n",
    "BATCH_SIZE = 32\n",
    "save_dir = '/opt/ml/v2/vo'\n",
    "age_dataloader = torch.utils.data.DataLoader(age_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "gender_dataloader = torch.utils.data.DataLoader(gender_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "mask_dataloader = torch.utils.data.DataLoader(mask_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 3. Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd34030d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6c2e5651ea34816b78989291fda97f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-0의 평균 Loss : 1.068, 평균 Accuracy : 0.674\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8a134164224f8883a31741a5dfdc30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-1의 평균 Loss : 0.213, 평균 Accuracy : 0.940\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abff7ccf9df641a592aede68337e236b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-2의 평균 Loss : 0.108, 평균 Accuracy : 0.969\n",
      "age 학습 종료!\n",
      "최고 accuracy : 0.9690476059913635, 최고 낮은 loss : 0.10788756691337135\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f211f2bae69d432786f04e5345f3cd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-0의 평균 Loss : 0.054, 평균 Accuracy : 0.981\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6560ec677f61472c867dbd047af86c87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-1의 평균 Loss : 0.020, 평균 Accuracy : 0.993\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d21992a96a254d3ea6309bd40abbb762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "현재 epoch-2의 평균 Loss : 0.017, 평균 Accuracy : 0.994\n",
      "gender 학습 종료!\n",
      "최고 accuracy : 0.9940860271453857, 최고 낮은 loss : 0.016571665326224325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81ad24cc61ad475eb35457320939f977",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1628.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 31.75 GiB total capacity; 22.85 GiB already allocated; 42.50 MiB free; 22.91 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/opt/ml/v2/ensemble.ipynb Cell 19'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.209.69/opt/ml/v2/ensemble.ipynb#ch0000018vscode-remote?line=66'>67</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.209.69/opt/ml/v2/ensemble.ipynb#ch0000018vscode-remote?line=67'>68</a>\u001b[0m optim_mask\u001b[39m.\u001b[39mzero_grad() \u001b[39m# parameter gradient를 업데이트 전 초기화함\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B101.101.209.69/opt/ml/v2/ensemble.ipynb#ch0000018vscode-remote?line=68'>69</a>\u001b[0m logits \u001b[39m=\u001b[39m net_mask(images)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.209.69/opt/ml/v2/ensemble.ipynb#ch0000018vscode-remote?line=69'>70</a>\u001b[0m _, preds \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmax(logits, \u001b[39m1\u001b[39m) \u001b[39m# 모델에서 linear 값으로 나오는 예측 값 ([0.9,1.2, 3.2,0.1,-0.1,...])을 최대 output index를 찾아 예측 레이블([2])로 변경함 \u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B101.101.209.69/opt/ml/v2/ensemble.ipynb#ch0000018vscode-remote?line=70'>71</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits, labels\u001b[39m.\u001b[39mlong())\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py:220\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=218'>219</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=219'>220</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward_impl(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py:211\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=208'>209</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=209'>210</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=210'>211</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer4(x)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=212'>213</a>\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=213'>214</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(x, \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py:117\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py?line=114'>115</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py?line=115'>116</a>\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py?line=116'>117</a>\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py?line=117'>118</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py:105\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=101'>102</a>\u001b[0m identity \u001b[39m=\u001b[39m x\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=103'>104</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x)\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=104'>105</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(out)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=105'>106</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torchvision/models/resnet.py?line=107'>108</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:727\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=724'>725</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=725'>726</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=726'>727</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=727'>728</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=728'>729</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=729'>730</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py?line=730'>731</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py:131\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=123'>124</a>\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=125'>126</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=126'>127</a>\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=127'>128</a>\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=128'>129</a>\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=129'>130</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=130'>131</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=131'>132</a>\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=132'>133</a>\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=133'>134</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=134'>135</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py?line=135'>136</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias, bn_training, exponential_average_factor, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:2056\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/functional.py?line=2052'>2053</a>\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/functional.py?line=2053'>2054</a>\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/functional.py?line=2055'>2056</a>\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/functional.py?line=2056'>2057</a>\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var,\n\u001b[1;32m   <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/functional.py?line=2057'>2058</a>\u001b[0m     training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   <a href='file:///opt/conda/lib/python3.8/site-packages/torch/nn/functional.py?line=2058'>2059</a>\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 48.00 MiB (GPU 0; 31.75 GiB total capacity; 22.85 GiB already allocated; 42.50 MiB free; 22.91 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "best_test_accuracy = 0.\n",
    "best_test_loss = 9999.\n",
    "for epoch in range(NUM_EPOCH):\n",
    "  running_loss = 0.\n",
    "  running_acc = 0.\n",
    "  net_age.train()\n",
    "  for ind, (images, labels) in enumerate(tqdm(age_dataloader)):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    optim_age.zero_grad() # parameter gradient를 업데이트 전 초기화함\n",
    "    logits = net_age(images)\n",
    "    _, preds = torch.max(logits, 1) # 모델에서 linear 값으로 나오는 예측 값 ([0.9,1.2, 3.2,0.1,-0.1,...])을 최대 output index를 찾아 예측 레이블([2])로 변경함 \n",
    "    loss = loss_fn(logits, labels.long())\n",
    "    loss.backward() # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient 계산\n",
    "    optim_age.step() # 계산된 gradient를 가지고 모델 업데이트\n",
    "    running_loss += loss.item() * images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "    running_acc += torch.sum(preds == labels.data) # 한 Batch에서의 Accuracy 값 저장\n",
    "  # 한 epoch이 모두 종료되었을 때,\n",
    "  epoch_loss = running_loss / len(age_dataloader.dataset)\n",
    "  epoch_acc = running_acc / len(age_dataloader.dataset)\n",
    "  print(f\"현재 epoch-{epoch}의 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}\")\n",
    "  if best_test_accuracy < epoch_acc: # phase가 test일 때, best accuracy 계산\n",
    "    best_test_accuracy = epoch_acc\n",
    "  if best_test_loss > epoch_loss: # phase가 test일 때, best loss 계산\n",
    "    best_test_loss = epoch_loss\n",
    "print(\"age 학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss : {best_test_loss}\")\n",
    "torch.save(net_age.state_dict(), f\"{save_dir}/age.pth\")\n",
    "\n",
    "best_test_accuracy = 0.\n",
    "best_test_loss = 9999.\n",
    "for epoch in range(NUM_EPOCH):\n",
    "  running_loss = 0.\n",
    "  running_acc = 0.\n",
    "  net_gender.train()\n",
    "  for ind, (images, labels) in enumerate(tqdm(gender_dataloader)):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    optim_gender.zero_grad() # parameter gradient를 업데이트 전 초기화함\n",
    "    logits = net_gender(images)\n",
    "    _, preds = torch.max(logits, 1) # 모델에서 linear 값으로 나오는 예측 값 ([0.9,1.2, 3.2,0.1,-0.1,...])을 최대 output index를 찾아 예측 레이블([2])로 변경함 \n",
    "    loss = loss_fn(logits, labels.long())\n",
    "    loss.backward() # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient 계산\n",
    "    optim_gender.step() # 계산된 gradient를 가지고 모델 업데이트\n",
    "    running_loss += loss.item() * images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "    running_acc += torch.sum(preds == labels.data) # 한 Batch에서의 Accuracy 값 저장\n",
    "  # 한 epoch이 모두 종료되었을 때,\n",
    "  epoch_loss = running_loss / len(gender_dataloader.dataset)\n",
    "  epoch_acc = running_acc / len(gender_dataloader.dataset)\n",
    "  print(f\"현재 epoch-{epoch}의 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}\")\n",
    "  if best_test_accuracy < epoch_acc: # phase가 test일 때, best accuracy 계산\n",
    "    best_test_accuracy = epoch_acc\n",
    "  if best_test_loss > epoch_loss: # phase가 test일 때, best loss 계산\n",
    "    best_test_loss = epoch_loss\n",
    "print(\"gender 학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss : {best_test_loss}\")\n",
    "torch.save(net_gender.state_dict(), f\"{save_dir}/gender.pth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3003824",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_test_accuracy = 0.\n",
    "best_test_loss = 9999.\n",
    "for epoch in range(NUM_EPOCH):\n",
    "  running_loss = 0.\n",
    "  running_acc = 0.\n",
    "  net_mask.train()\n",
    "  for ind, (images, labels) in enumerate(tqdm(mask_dataloader)):\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    optim_mask.zero_grad() # parameter gradient를 업데이트 전 초기화함\n",
    "    logits = net_mask(images)\n",
    "    _, preds = torch.max(logits, 1) # 모델에서 linear 값으로 나오는 예측 값 ([0.9,1.2, 3.2,0.1,-0.1,...])을 최대 output index를 찾아 예측 레이블([2])로 변경함 \n",
    "    loss = loss_fn(logits, labels.long())\n",
    "    loss.backward() # 모델의 예측 값과 실제 값의 CrossEntropy 차이를 통해 gradient 계산\n",
    "    optim_mask.step() # 계산된 gradient를 가지고 모델 업데이트\n",
    "    running_loss += loss.item() * images.size(0) # 한 Batch에서의 loss 값 저장\n",
    "    running_acc += torch.sum(preds == labels.data) # 한 Batch에서의 Accuracy 값 저장\n",
    "  # 한 epoch이 모두 종료되었을 때,\n",
    "  epoch_loss = running_loss / len(mask_dataloader.dataset)\n",
    "  epoch_acc = running_acc / len(mask_dataloader.dataset)\n",
    "  print(f\"현재 epoch-{epoch}의 평균 Loss : {epoch_loss:.3f}, 평균 Accuracy : {epoch_acc:.3f}\")\n",
    "  if best_test_accuracy < epoch_acc: # phase가 test일 때, best accuracy 계산\n",
    "    best_test_accuracy = epoch_acc\n",
    "  if best_test_loss > epoch_loss: # phase가 test일 때, best loss 계산\n",
    "    best_test_loss = epoch_loss\n",
    "print(\"mask 학습 종료!\")\n",
    "print(f\"최고 accuracy : {best_test_accuracy}, 최고 낮은 loss : {best_test_loss}\")\n",
    "torch.save(net_mask.state_dict(), f\"{save_dir}/mask.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db477eb",
   "metadata": {},
   "source": [
    "## 4.Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7494dfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6227ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42ff15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a1ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "transform = transforms.Compose([\n",
    "    Resize((512, 384), Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])\n",
    "dataset = TestDataset(image_paths, transform)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# 모델을 정의합니다. (학습한 모델이 있다면 torch.load로 모델을 불러주세요!)\n",
    "device = torch.device('cuda')\n",
    "net_age.eval()\n",
    "net_gender.eval()\n",
    "net_mask.eval()\n",
    "\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "preds_age = []\n",
    "preds_gender = []\n",
    "preds_mask = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = net_age(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        preds_age.extend(pred.cpu().numpy())\n",
    "\n",
    "        pred = net_gender(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        preds_gender.extend(pred.cpu().numpy())\n",
    "\n",
    "        pred = net_mask(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        preds_mask.extend(pred.cpu().numpy())\n",
    "\n",
    "values = list(zip(preds_age,preds_gender,preds_mask))\n",
    "answers = []\n",
    "for value in values:\n",
    "    print(value)\n",
    "    age, gender, mask = value\n",
    "    if age < 30 :\n",
    "            c = 0\n",
    "    elif 30 <= age < 60:\n",
    "        c = 1\n",
    "    else:\n",
    "        c = 2\n",
    "    if gender == 1:\n",
    "        c += 3\n",
    "    if mask == 1:\n",
    "        c += 6\n",
    "    elif mask == 2:\n",
    "        c += 12\n",
    "    print(f'age : {age}, gender: {gender}, mask : {mask}, class : {c}')\n",
    "    answers.append(c)\n",
    "submission['ans'] = answers\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(test_dir, 'submission4.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea6457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
