{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import *\n",
    "import torchvision.models as models\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import albumentations\n",
    "import albumentations.pytorch.transforms\n",
    "from torchsampler import ImbalancedDatasetSampler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 1. Train Dataset 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575fde9",
   "metadata": {},
   "source": [
    "### age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, train_dir, transform):\n",
    "        self.transform = transform\n",
    "        profiles = os.listdir(train_dir)\n",
    "        indices = []\n",
    "        for profile in profiles:\n",
    "            if profile.startswith('.'):\n",
    "                continue\n",
    "\n",
    "            id, gender, race, age = profile.split('_')\n",
    "            gender = 0 if gender == 'male' else 1\n",
    "            \n",
    "            age = int(age)\n",
    "            if age < 30 :\n",
    "                age = 0\n",
    "            elif 30 <= age < 60:\n",
    "                age = 1\n",
    "            else:\n",
    "                age = 2\n",
    "            \n",
    "            names = os.listdir(os.path.join(train_dir, profile))\n",
    "            for name in names:\n",
    "                if name.startswith('.'):\n",
    "                    continue\n",
    "\n",
    "                if name.startswith('mask'):\n",
    "                    mask = 0\n",
    "                elif name.startswith('in'):\n",
    "                    mask = 1\n",
    "                else:\n",
    "                    mask = 2\n",
    "                name_path = os.path.join(train_dir, profile, name)\n",
    "                indices.append((name_path, gender, age, mask))\n",
    "        ## profiles_df\n",
    "        # gender: male -> 0, female -> 1\n",
    "        # mask: mask -> 0, incorrect -> 1, normal -> 2\n",
    "        profiles_df = pd.DataFrame(indices, columns=['path', 'gender', 'age', 'mask'])\n",
    "        self.image_path = profiles_df.path\n",
    "        self.image_labels = profiles_df.age\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_path[index])\n",
    "        if self.transform:\n",
    "            if isinstance(self.transform, transforms.Compose):\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = self.transform(image=np.array(image))['image']\n",
    "            \n",
    "        return image, int(self.image_labels[index])\n",
    "    \n",
    "    def split_dataset(self) -> Tuple[Subset, Subset]:\n",
    "        n_val = int(len(self.image_path) * 0.2)\n",
    "        val_indices = set(random.sample(range(len(self.image_path)), k=n_val))  \n",
    "        train_indices = set(range(len(self.image_path))) - val_indices\n",
    "\n",
    "        return Subset(self, list(train_indices)), Subset(self, list(val_indices))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_labels) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d8f33",
   "metadata": {},
   "source": [
    "### gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0914be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, train_dir, transform):\n",
    "        self.transform = transform\n",
    "        profiles = os.listdir(train_dir)\n",
    "        indices = []\n",
    "        for profile in profiles:\n",
    "            if profile.startswith('.'):\n",
    "                continue\n",
    "\n",
    "            id, gender, race, age = profile.split('_')\n",
    "            gender = 0 if gender == 'male' else 1\n",
    "            \n",
    "            age = int(age)\n",
    "            if age < 30 :\n",
    "                age = 0\n",
    "            elif 30 <= age < 60:\n",
    "                age = 1\n",
    "            else:\n",
    "                age = 2\n",
    "            \n",
    "            names = os.listdir(os.path.join(train_dir, profile))\n",
    "            for name in names:\n",
    "                if name.startswith('.'):\n",
    "                    continue\n",
    "\n",
    "                if name.startswith('mask'):\n",
    "                    mask = 0\n",
    "                elif name.startswith('in'):\n",
    "                    mask = 1\n",
    "                else:\n",
    "                    mask = 2\n",
    "                name_path = os.path.join(train_dir, profile, name)\n",
    "                indices.append((name_path, gender, age, mask))\n",
    "        ## profiles_df\n",
    "        # gender: male -> 0, female -> 1\n",
    "        # mask: mask -> 0, incorrect -> 1, normal -> 2\n",
    "        profiles_df = pd.DataFrame(indices, columns=['path', 'gender', 'age', 'mask'])\n",
    "        self.image_path = profiles_df.path\n",
    "        self.image_labels = profiles_df.gender\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_path[index])\n",
    "        if self.transform:\n",
    "            if isinstance(self.transform, transforms.Compose):\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = self.transform(image=np.array(image))['image']\n",
    "            \n",
    "        return image, int(self.image_labels[index])\n",
    "    \n",
    "    def split_dataset(self) -> Tuple[Subset, Subset]:\n",
    "        n_val = int(len(self.image_path) * 0.2)\n",
    "        val_indices = set(random.sample(range(len(self.image_path)), k=n_val))  \n",
    "        train_indices = set(range(len(self.image_path))) - val_indices\n",
    "\n",
    "        return Subset(self, list(train_indices)), Subset(self, list(val_indices))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932fa84",
   "metadata": {},
   "source": [
    "### mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fac4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, train_dir, transform):\n",
    "        self.transform = transform\n",
    "        profiles = os.listdir(train_dir)\n",
    "        indices = []\n",
    "        for profile in profiles:\n",
    "            if profile.startswith('.'):\n",
    "                continue\n",
    "\n",
    "            id, gender, race, age = profile.split('_')\n",
    "            gender = 0 if gender == 'male' else 1\n",
    "            \n",
    "            age = int(age)\n",
    "            if age < 30 :\n",
    "                age = 0\n",
    "            elif 30 <= age < 60:\n",
    "                age = 1\n",
    "            else:\n",
    "                age = 2\n",
    "            \n",
    "            names = os.listdir(os.path.join(train_dir, profile))\n",
    "            for name in names:\n",
    "                if name.startswith('.'):\n",
    "                    continue\n",
    "\n",
    "                if name.startswith('mask'):\n",
    "                    masked = 0\n",
    "                elif name.startswith('in'):\n",
    "                    masked = 1\n",
    "                else:\n",
    "                    masked = 2\n",
    "                name_path = os.path.join(train_dir, profile, name)\n",
    "                indices.append((name_path, gender, age, masked))\n",
    "        ## profiles_df\n",
    "        # gender: male -> 0, female -> 1\n",
    "        # masked: mask -> 0, incorrect -> 1, normal -> 2\n",
    "        profiles_df = pd.DataFrame(indices, columns=['path', 'gender', 'age', 'masked'])\n",
    "        self.image_path = profiles_df.path\n",
    "        self.image_labels = profiles_df.masked\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.image_path[index])\n",
    "        if self.transform:\n",
    "            if isinstance(self.transform, transforms.Compose):\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = self.transform(image=np.array(image))['image']\n",
    "            \n",
    "        return image, int(self.image_labels[index])\n",
    "    \n",
    "    def split_dataset(self) -> Tuple[Subset, Subset]:\n",
    "        n_val = int(len(self.image_path) * 0.2)\n",
    "        val_indices = set(random.sample(range(len(self.image_path)), k=n_val))  \n",
    "        train_indices = set(range(len(self.image_path))) - val_indices\n",
    "\n",
    "        return Subset(self, list(train_indices)), Subset(self, list(val_indices))\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a50e90-3bfd-4a12-bbff-8ca07cab69e0",
   "metadata": {},
   "source": [
    "### transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3dd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --image_size\n",
    "image_size = (512, 384)\n",
    "\n",
    "# --transforms\n",
    "transform_train = albumentations.Compose([\n",
    "    albumentations.Resize(height=image_size[0], width=image_size[1], always_apply=True),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.HorizontalFlip(p=0.9),  # 좌우 반전\n",
    "        albumentations.VerticalFlip(p=0.9),  # 상하 반전,\n",
    "        albumentations.Affine(p=0.9),\n",
    "        albumentations.ShiftScaleRotate(\n",
    "            shift_limit=0.2,\n",
    "            scale_limit=0.2,\n",
    "            rotate_limit=10,\n",
    "            border_mode=0,\n",
    "            p=0.9),\n",
    "    ], p=0),  # p=0, for cutmix\n",
    "    albumentations.GaussNoise(p=0.4),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.MotionBlur(p=0.9),\n",
    "        albumentations.MedianBlur(blur_limit=3, p=0.9),\n",
    "        albumentations.Blur(blur_limit=3, p=0.9),\n",
    "    ], p=1),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.HueSaturationValue(p=0.9),\n",
    "        albumentations.RGBShift(p=0.9),\n",
    "        albumentations.ChannelShuffle(p=0.9),\n",
    "        albumentations.ColorJitter(p=0.9),\n",
    "    ], p=1),\n",
    "\n",
    "    albumentations.CoarseDropout(max_holes=4,max_height=30,max_width=30,p=0.2),\n",
    "    albumentations.RandomBrightnessContrast(p=0.4),\n",
    "\n",
    "    albumentations.Normalize(mean=(0.5,0.5,0.5), std=(0.2,0.2,0.2)), \n",
    "    albumentations.pytorch.transforms.ToTensorV2(p=1)\n",
    "    ], p=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820a8ce-fa4f-441c-a1d3-01cc763dfd08",
   "metadata": {},
   "source": [
    "### train setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c388e65-ff5d-4bb0-8b2f-28bfda76df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "627e3e98-35c4-4c7c-9699-cdf9aea873e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find bbox for cutmix\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]  # batch, channel, width, height\n",
    "    H = size[3]  # batch, channel, width, height\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (size[2] * size[3]))\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "714d8abc-75d4-462a-9f6a-5f642da247f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --basic settings\n",
    "train_dir = '/opt/ml/input/data/train/images'\n",
    "seed_everything(42)  # seed: 42\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# --train setting\n",
    "NUM_EPOCH = 10\n",
    "BATCH_SIZE = 64\n",
    "save_dir = os.path.join('/opt/ml/code/baselinecode_v1/model', '13_ensemble_sampler')\n",
    "log_interval = 100\n",
    "patience = 2  \n",
    "\n",
    "# --dataset & augmentation\n",
    "age_dataset = AgeDataset(train_dir,transform_train)\n",
    "age_train, age_val = age_dataset.split_dataset()\n",
    "\n",
    "gender_dataset = GenderDataset(train_dir,transform_train)\n",
    "gender_train, gender_val = gender_dataset.split_dataset()\n",
    "\n",
    "mask_dataset = MaskDataset(train_dir,transform_train)\n",
    "mask_train, mask_val = mask_dataset.split_dataset()\n",
    "\n",
    "# --dataloader\n",
    "age_train_loader = torch.utils.data.DataLoader(\n",
    "    age_train, batch_size=BATCH_SIZE, sampler=ImbalancedDatasetSampler(age_train), num_workers=2\n",
    "    )\n",
    "age_val_loader = torch.utils.data.DataLoader(age_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "gender_train_loader = torch.utils.data.DataLoader(\n",
    "    gender_train, batch_size=BATCH_SIZE, sampler=ImbalancedDatasetSampler(gender_train), num_workers=2\n",
    "    )\n",
    "gender_val_loader = torch.utils.data.DataLoader(gender_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "mask_train_loader = torch.utils.data.DataLoader(\n",
    "    mask_train, batch_size=BATCH_SIZE, sampler=ImbalancedDatasetSampler(mask_train), num_workers=2\n",
    "    )\n",
    "mask_val_loader = torch.utils.data.DataLoader(mask_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "\n",
    "# --load models\n",
    "age_model = models.resnet18(pretrained=True)\n",
    "gender_model = models.resnet18(pretrained=True)\n",
    "mask_model = models.resnet18(pretrained=True)\n",
    "\n",
    "# --initialize models\n",
    "age_model.fc = torch.nn.Linear(in_features=512, out_features=3, bias=True)\n",
    "torch.nn.init.kaiming_normal_(age_model.fc.weight)\n",
    "age_model.fc.bias.data.fill_(0.)\n",
    "\n",
    "gender_model.fc = torch.nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "torch.nn.init.kaiming_normal_(gender_model.fc.weight)\n",
    "gender_model.fc.bias.data.fill_(0.)\n",
    "\n",
    "mask_model.fc = torch.nn.Linear(in_features=512, out_features=3, bias=True)\n",
    "torch.nn.init.kaiming_normal_(mask_model.fc.weight)\n",
    "mask_model.fc.bias.data.fill_(0.)\n",
    "\n",
    "# --loss & metric\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "optim_age = torch.optim.Adam(age_model.parameters(), lr=LEARNING_RATE)\n",
    "optim_gender = torch.optim.Adam(gender_model.parameters(), lr=LEARNING_RATE)\n",
    "optim_mask = torch.optim.Adam(mask_model.parameters(), lr=LEARNING_RATE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbc75c2e-ecf0-420c-b4a6-542497838372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "폴더 생성 완료\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(save_dir):\n",
    "    print('해당 폴더가 이미 존재합니다.')\n",
    "else:\n",
    "    os.makedirs(save_dir)\n",
    "    print('폴더 생성 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dadc80-011f-41a4-8003-ec3b155113d4",
   "metadata": {},
   "source": [
    "### age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fd34030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train loop start !]\n",
      "Epoch[0/10](100/237) || training loss 0.5956 || training accuracy 74.19% || lr 0.0001\n",
      "Epoch[0/10](200/237) || training loss 0.4098 || training accuracy 85.83% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 87.99%! saving the best model..\n",
      "[Val] acc : 87.99%, loss: 0.31 || best acc : 87.99%, best loss: 0.31\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[1/10](100/237) || training loss 0.3601 || training accuracy 86.58% || lr 0.0001\n",
      "Epoch[1/10](200/237) || training loss 0.3166 || training accuracy 92.03% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 97.99%! saving the best model..\n",
      "[Val] acc : 97.99%, loss: 0.11 || best acc : 97.99%, best loss: 0.11\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[2/10](100/237) || training loss 0.3191 || training accuracy 89.19% || lr 0.0001\n",
      "Epoch[2/10](200/237) || training loss 0.3178 || training accuracy 90.31% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 98.60%! saving the best model..\n",
      "[Val] acc : 98.60%, loss: 0.07 || best acc : 98.60%, best loss: 0.07\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[3/10](100/237) || training loss 0.2807 || training accuracy 90.41% || lr 0.0001\n",
      "Epoch[3/10](200/237) || training loss 0.2992 || training accuracy 91.98% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.02%! saving the best model..\n",
      "[Val] acc : 99.02%, loss: 0.07 || best acc : 99.02%, best loss: 0.07\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[4/10](100/237) || training loss 0.2525 || training accuracy 92.59% || lr 0.0001\n",
      "Epoch[4/10](200/237) || training loss 0.2384 || training accuracy 93.16% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "[Val] acc : 98.78%, loss: 0.077 || best acc : 99.02%, best loss: 0.07\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[5/10](100/237) || training loss 0.3036 || training accuracy 91.19% || lr 0.0001\n",
      "Epoch[5/10](200/237) || training loss 0.2333 || training accuracy 91.33% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.37%! saving the best model..\n",
      "[Val] acc : 99.37%, loss: 0.049 || best acc : 99.37%, best loss: 0.049\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[6/10](100/237) || training loss 0.261 || training accuracy 91.59% || lr 0.0001\n",
      "Epoch[6/10](200/237) || training loss 0.2585 || training accuracy 92.89% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.50%! saving the best model..\n",
      "[Val] acc : 99.50%, loss: 0.055 || best acc : 99.50%, best loss: 0.049\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[7/10](100/237) || training loss 0.2958 || training accuracy 90.00% || lr 0.0001\n",
      "Epoch[7/10](200/237) || training loss 0.2885 || training accuracy 88.70% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.74%! saving the best model..\n",
      "[Val] acc : 99.74%, loss: 0.041 || best acc : 99.74%, best loss: 0.041\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[8/10](100/237) || training loss 0.2818 || training accuracy 90.81% || lr 0.0001\n",
      "Epoch[8/10](200/237) || training loss 0.2692 || training accuracy 92.06% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "[Val] acc : 99.15%, loss: 0.075 || best acc : 99.74%, best loss: 0.041\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[9/10](100/237) || training loss 0.2258 || training accuracy 91.22% || lr 0.0001\n",
      "Epoch[9/10](200/237) || training loss 0.2489 || training accuracy 91.78% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "[Val] acc : 99.26%, loss: 0.046 || best acc : 99.74%, best loss: 0.041\n",
      "\n"
     ]
    }
   ],
   "source": [
    "age_model.to(device)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "counter = 0\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # train loop\n",
    "    age_model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    cutmix_prob = 0.5  # cutmix 확률을 조절합니다. 0으로 하면, 실행하지 않습니다.\n",
    "    print('[train loop start !]')\n",
    "    for idx, train_batch in enumerate(age_train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optim_age.zero_grad()\n",
    "\n",
    "        ## cutmix part start ## \n",
    "        r = np.random.rand(1)\n",
    "        if np.random.rand(1) < cutmix_prob:\n",
    "            lam = np.random.beta(1.0, 1.0)  # 베타분포는 알파베타가 1이면, uniform분포가 됨\n",
    "            rand_index = torch.randperm(inputs.size()[0]).to(device)\n",
    "            label_a = labels\n",
    "            label_b = labels[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2, lam = rand_bbox(inputs.size(), lam) \n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            outs = age_model(inputs)\n",
    "            loss = lam * loss_fn(outs, label_a) + (1. - lam) * loss_fn(outs, label_b)  # mix_target\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "        ## cutmix part done ##\n",
    "        else:\n",
    "            outs = age_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = loss_fn(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optim_age.step()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % log_interval == 0:\n",
    "            train_loss = loss_value / log_interval\n",
    "            train_acc = matches / BATCH_SIZE / log_interval\n",
    "            current_lr = LEARNING_RATE\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{NUM_EPOCH}]({idx + 1}/{len(age_train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "    \n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"[Calculating validation results...]\")\n",
    "        age_model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        figure = None\n",
    "        for val_batch in age_val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = age_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = loss_fn(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(age_val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(age_val)\n",
    "        best_val_loss = min(best_val_loss, val_loss)\n",
    "        if val_acc > best_val_acc:\n",
    "            print(f\"New best model for val accuracy : {val_acc:4.2%}! saving the best model..\")\n",
    "            torch.save(age_model.state_dict(), f\"{save_dir}/age_best.pth\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # early stopping\n",
    "        if counter > patience:\n",
    "            print('Early Stopping...')\n",
    "            break\n",
    "\n",
    "        torch.save(age_model.state_dict(), f\"{save_dir}/age_last.pth\")\n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ea911-4a2b-426a-a816-ff7c433d05f3",
   "metadata": {},
   "source": [
    "### gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "118cf1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b60280c7-f276-4b0e-a54f-d15a4107c7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train loop start !]\n",
      "Epoch[0/10](100/237) || training loss 0.36 || training accuracy 84.38% || lr 0.0001\n",
      "Epoch[0/10](200/237) || training loss 0.24 || training accuracy 91.23% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 97.94%! saving the best model..\n",
      "[Val] acc : 97.94%, loss: 0.081 || best acc : 97.94%, best loss: 0.081\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[1/10](100/237) || training loss 0.2334 || training accuracy 90.62% || lr 0.0001\n",
      "Epoch[1/10](200/237) || training loss 0.2039 || training accuracy 94.50% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.42%! saving the best model..\n",
      "[Val] acc : 99.42%, loss: 0.047 || best acc : 99.42%, best loss: 0.047\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[2/10](100/237) || training loss 0.214 || training accuracy 91.78% || lr 0.0001\n",
      "Epoch[2/10](200/237) || training loss 0.206 || training accuracy 92.22% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.87%! saving the best model..\n",
      "[Val] acc : 99.87%, loss: 0.035 || best acc : 99.87%, best loss: 0.035\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[3/10](100/237) || training loss 0.1645 || training accuracy 93.56% || lr 0.0001\n",
      "Epoch[3/10](200/237) || training loss 0.189 || training accuracy 94.16% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "[Val] acc : 99.81%, loss: 0.033 || best acc : 99.87%, best loss: 0.033\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[4/10](100/237) || training loss 0.1532 || training accuracy 94.69% || lr 0.0001\n",
      "Epoch[4/10](200/237) || training loss 0.1472 || training accuracy 94.91% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "[Val] acc : 99.87%, loss: 0.022 || best acc : 99.87%, best loss: 0.022\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[5/10](100/237) || training loss 0.1923 || training accuracy 93.97% || lr 0.0001\n",
      "Epoch[5/10](200/237) || training loss 0.1496 || training accuracy 93.50% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "Early Stopping...\n"
     ]
    }
   ],
   "source": [
    "gender_model.to(device)\n",
    "\n",
    "best_val_acc = 0.\n",
    "best_val_loss = np.inf\n",
    "counter = 0\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # train loop\n",
    "    gender_model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    cutmix_prob = 0.5  # cutmix 확률을 조절합니다. 0으로 하면, 실행하지 않습니다.\n",
    "    print('[train loop start !]')\n",
    "    for idx, train_batch in enumerate(gender_train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optim_gender.zero_grad()\n",
    "\n",
    "        ## cutmix part start ## \n",
    "        r = np.random.rand(1)\n",
    "        if np.random.rand(1) < cutmix_prob:\n",
    "            lam = np.random.beta(1.0, 1.0)  # 베타분포는 알파베타가 1이면, uniform분포가 됨\n",
    "            rand_index = torch.randperm(inputs.size()[0]).to(device)\n",
    "            label_a = labels\n",
    "            label_b = labels[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2, lam = rand_bbox(inputs.size(), lam) \n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            outs = gender_model(inputs)\n",
    "            loss = lam * loss_fn(outs, label_a) + (1. - lam) * loss_fn(outs, label_b)  # mix_target\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "        ## cutmix part done ##\n",
    "        else:\n",
    "            outs = gender_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = loss_fn(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optim_gender.step()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % log_interval == 0:\n",
    "            train_loss = loss_value / log_interval\n",
    "            train_acc = matches / BATCH_SIZE / log_interval\n",
    "            current_lr = LEARNING_RATE\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{NUM_EPOCH}]({idx + 1}/{len(gender_train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "        \n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"[Calculating validation results...]\")\n",
    "        gender_model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        figure = None\n",
    "        for val_batch in gender_val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = gender_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = loss_fn(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(gender_val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(gender_val)\n",
    "        best_val_loss = min(best_val_loss, val_loss)\n",
    "        if val_acc > best_val_acc:\n",
    "            print(f\"New best model for val accuracy : {val_acc:4.2%}! saving the best model..\")\n",
    "            torch.save(gender_model.state_dict(), f\"{save_dir}/gender_best.pth\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # early stopping\n",
    "        if counter > patience:\n",
    "            print('Early Stopping...')\n",
    "            break\n",
    "\n",
    "        torch.save(gender_model.state_dict(), f\"{save_dir}/gender_last.pth\")\n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )\n",
    "        print()\n",
    "        print('Calculating validation done !')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ab54b-7352-416e-95a2-f49fe2f1b787",
   "metadata": {},
   "source": [
    "### mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3003824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train loop start !]\n",
      "Epoch[0/10](100/237) || training loss 0.5429 || training accuracy 77.38% || lr 0.0001\n",
      "Epoch[0/10](200/237) || training loss 0.3737 || training accuracy 86.00% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.55%! saving the best model..\n",
      "[Val] acc : 99.55%, loss: 0.09 || best acc : 99.55%, best loss: 0.09\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[1/10](100/237) || training loss 0.3464 || training accuracy 84.47% || lr 0.0001\n",
      "Epoch[1/10](200/237) || training loss 0.291 || training accuracy 91.06% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.68%! saving the best model..\n",
      "[Val] acc : 99.68%, loss: 0.042 || best acc : 99.68%, best loss: 0.042\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[2/10](100/237) || training loss 0.3185 || training accuracy 85.33% || lr 0.0001\n",
      "Epoch[2/10](200/237) || training loss 0.3106 || training accuracy 86.89% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.74%! saving the best model..\n",
      "[Val] acc : 99.74%, loss: 0.022 || best acc : 99.74%, best loss: 0.022\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[3/10](100/237) || training loss 0.2675 || training accuracy 89.58% || lr 0.0001\n",
      "Epoch[3/10](200/237) || training loss 0.2941 || training accuracy 88.69% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.76%! saving the best model..\n",
      "[Val] acc : 99.76%, loss: 0.075 || best acc : 99.76%, best loss: 0.022\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[4/10](100/237) || training loss 0.2398 || training accuracy 90.77% || lr 0.0001\n",
      "Epoch[4/10](200/237) || training loss 0.2332 || training accuracy 90.52% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "[Val] acc : 99.76%, loss: 0.046 || best acc : 99.76%, best loss: 0.022\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[5/10](100/237) || training loss 0.3113 || training accuracy 88.84% || lr 0.0001\n",
      "Epoch[5/10](200/237) || training loss 0.2392 || training accuracy 89.05% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.81%! saving the best model..\n",
      "[Val] acc : 99.81%, loss: 0.031 || best acc : 99.81%, best loss: 0.022\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[6/10](100/237) || training loss 0.2773 || training accuracy 87.72% || lr 0.0001\n",
      "Epoch[6/10](200/237) || training loss 0.2784 || training accuracy 89.94% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.89%! saving the best model..\n",
      "[Val] acc : 99.89%, loss: 0.029 || best acc : 99.89%, best loss: 0.022\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[7/10](100/237) || training loss 0.307 || training accuracy 87.30% || lr 0.0001\n",
      "Epoch[7/10](200/237) || training loss 0.3081 || training accuracy 84.47% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "[Val] acc : 99.81%, loss: 0.036 || best acc : 99.89%, best loss: 0.022\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[8/10](100/237) || training loss 0.301 || training accuracy 86.70% || lr 0.0001\n",
      "Epoch[8/10](200/237) || training loss 0.2846 || training accuracy 88.88% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "[Val] acc : 99.81%, loss: 0.043 || best acc : 99.89%, best loss: 0.022\n",
      "\n",
      "Calculating validation done !\n",
      "\n",
      "[train loop start !]\n",
      "Epoch[9/10](100/237) || training loss 0.2408 || training accuracy 89.14% || lr 0.0001\n",
      "Epoch[9/10](200/237) || training loss 0.2629 || training accuracy 90.05% || lr 0.0001\n",
      "[Calculating validation results...]\n",
      "New best model for val accuracy : 99.92%! saving the best model..\n",
      "[Val] acc : 99.92%, loss: 0.026 || best acc : 99.92%, best loss: 0.022\n",
      "\n",
      "Calculating validation done !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mask_model.to(device)\n",
    "\n",
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "counter = 0\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # train loop\n",
    "    mask_model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    cutmix_prob = 0.5  # cutmix 확률을 조절합니다. 0으로 하면, 실행하지 않습니다.\n",
    "    print('[train loop start !]')\n",
    "    for idx, train_batch in enumerate(mask_train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optim_mask.zero_grad()\n",
    "\n",
    "        ## cutmix part start ## \n",
    "        r = np.random.rand(1)\n",
    "        if np.random.rand(1) < cutmix_prob:\n",
    "            lam = np.random.beta(1.0, 1.0)  # 베타분포는 알파베타가 1이면, uniform분포가 됨\n",
    "            rand_index = torch.randperm(inputs.size()[0]).to(device)\n",
    "            label_a = labels\n",
    "            label_b = labels[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2, lam = rand_bbox(inputs.size(), lam) \n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            outs = mask_model(inputs)\n",
    "            loss = lam * loss_fn(outs, label_a) + (1. - lam) * loss_fn(outs, label_b)  # mix_target\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "        ## cutmix part done ##\n",
    "        else:\n",
    "            outs = mask_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = loss_fn(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optim_mask.step()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % log_interval == 0:\n",
    "            train_loss = loss_value / log_interval\n",
    "            train_acc = matches / BATCH_SIZE / log_interval\n",
    "            current_lr = LEARNING_RATE\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{NUM_EPOCH}]({idx + 1}/{len(mask_train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "        \n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"[Calculating validation results...]\")\n",
    "        mask_model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        figure = None\n",
    "        for val_batch in mask_val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = mask_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = loss_fn(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(mask_val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(mask_val)\n",
    "        best_val_loss = min(best_val_loss, val_loss)\n",
    "        if val_acc > best_val_acc:\n",
    "            print(f\"New best model for val accuracy : {val_acc:4.2%}! saving the best model..\")\n",
    "            torch.save(mask_model.state_dict(), f\"{save_dir}/mask_best.pth\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # early stopping\n",
    "        if counter > patience:\n",
    "            print('Early Stopping...')\n",
    "            break\n",
    "\n",
    "        torch.save(mask_model.state_dict(), f\"{save_dir}/mask_last.pth\")\n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )\n",
    "        print()\n",
    "        print('Calculating validation done !')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db477eb",
   "metadata": {},
   "source": [
    "## 3.Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdbfd4-c625-4ff0-a3ec-7084dbdcf26e",
   "metadata": {},
   "source": [
    "### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c06ae05-b519-4b88-a3a6-d6e9f2421057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "040ea473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --image_size\n",
    "image_size = (512, 384)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    Resize(image_size, Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9ad87-598b-46c0-a55d-2260964b24e4",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d1d4c82-8cd0-4fd6-a04c-27d276635e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-314262fc10f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mpreds_gender\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mpreds_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-13a98d74e549>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m         \"\"\"\n\u001b[0;32m--> 226\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mtensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -- basic setting\n",
    "seed_everything(42)  # seed: 42\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "dataset = TestDataset(image_paths, transform_test)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# --load models\n",
    "age_model = models.resnet18(pretrained=False)\n",
    "gender_model = models.resnet18(pretrained=False)\n",
    "mask_model = models.resnet18(pretrained=False)\n",
    "\n",
    "age_model.fc = torch.nn.Linear(in_features=512, out_features=3, bias=True)\n",
    "gender_model.fc = torch.nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "mask_model.fc = torch.nn.Linear(in_features=512, out_features=3, bias=True)\n",
    "\n",
    "age_model.load_state_dict(torch.load('/opt/ml/code/baselinecode_v1/model/13_ensemble_sampler/age_best.pth', map_location=device))\n",
    "age_model.to(device)\n",
    "gender_model.load_state_dict(torch.load('/opt/ml/code/baselinecode_v1/model/13_ensemble_sampler/gender_best.pth', map_location=device))\n",
    "gender_model.to(device)\n",
    "mask_model.load_state_dict(torch.load('/opt/ml/code/baselinecode_v1/model/13_ensemble_sampler/mask_best.pth', map_location=device))\n",
    "mask_model.to(device)\n",
    "\n",
    "age_model.eval()\n",
    "gender_model.eval()\n",
    "mask_model.eval()\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "preds_age = []\n",
    "preds_gender = []\n",
    "preds_mask = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = age_model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        preds_age.extend(pred.cpu().numpy())\n",
    "\n",
    "        pred = gender_model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        preds_gender.extend(pred.cpu().numpy())\n",
    "\n",
    "        pred = mask_model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        preds_mask.extend(pred.cpu().numpy())\n",
    "\n",
    "values = list(zip(preds_age,preds_gender,preds_mask))\n",
    "answers = []\n",
    "for value in values:\n",
    "    age, gender, mask = value\n",
    "    c= 0\n",
    "    c += age\n",
    "    if gender == 1:\n",
    "        c += 3\n",
    "    if mask == 1:\n",
    "        c += 6\n",
    "    elif mask == 2:\n",
    "        c += 12\n",
    "    #print(f'age : {age}, gender: {gender}, mask : {mask}, class : {c}')\n",
    "    answers.append(c)\n",
    "submission['ans'] = answers\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join('/opt/ml/code/baselinecode_v1/output', 'output.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a03e47",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
