{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "about-heavy",
   "metadata": {},
   "source": [
    "## 0. Libarary 불러오기 및 경로설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cubic-scoop",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Tuple, List\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import *\n",
    "import torchvision.models as models\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import albumentations\n",
    "import albumentations.pytorch.transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "domestic-channels",
   "metadata": {},
   "source": [
    "## 1. Train Dataset 정의"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b575fde9",
   "metadata": {},
   "source": [
    "### age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "extensive-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgeDataset(Dataset):\n",
    "    def __init__(self, train_path, transform):\n",
    "        self.transform = transform\n",
    "        profiles = os.listdir(train_dir)\n",
    "        indices = []\n",
    "        for profile in profiles:\n",
    "            if profile.startswith('.'):\n",
    "                continue\n",
    "\n",
    "            id, gender, race, age = profile.split('_')\n",
    "            gender = 0 if gender == 'male' else 1\n",
    "\n",
    "            names = os.listdir(os.path.join(train_dir, profile))\n",
    "            for name in names:\n",
    "                if name.startswith('.'):\n",
    "                    continue\n",
    "\n",
    "                if name.startswith('mask'):\n",
    "                    mask = 0\n",
    "                elif name.startswith('in'):\n",
    "                    mask = 1\n",
    "                else:\n",
    "                    mask = 2\n",
    "                name_path = os.path.join(train_dir, profile, name)\n",
    "                indices.append((name_path, gender, age, mask))\n",
    "        ## profiles_df\n",
    "        # gender: male -> 0, female -> 1\n",
    "        # mask: mask -> 0, incorrect -> 1, normal -> 2\n",
    "        profiles_df = pd.DataFrame(indices, columns=['path', 'gender', 'age', 'mask'])\n",
    "        self.X = profiles_df.path\n",
    "        self.Y = profiles_df.age\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.X[index])\n",
    "        if self.transform:\n",
    "            if isinstance(self.transform, transforms.Compose):\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = self.transform(image=np.array(image))['image']\n",
    "            \n",
    "        return image, int(self.Y[index])\n",
    "    \n",
    "    def split_dataset(self) -> Tuple[Subset, Subset]:\n",
    "        n_val = int(len(self.X) * 0.2)\n",
    "        n_train = len(self.X) - n_val\n",
    "        train_set, val_set = random_split(self, [n_train, n_val])\n",
    "        return train_set, val_set\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971d8f33",
   "metadata": {},
   "source": [
    "### gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0914be15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenderDataset(Dataset):\n",
    "    def __init__(self, train_path, transform):\n",
    "        self.transform = transform\n",
    "        profiles = os.listdir(train_dir)\n",
    "        indices = []\n",
    "        for profile in profiles:\n",
    "            if profile.startswith('.'):\n",
    "                continue\n",
    "\n",
    "            id, gender, race, age = profile.split('_')\n",
    "            gender = 0 if gender == 'male' else 1\n",
    "\n",
    "            names = os.listdir(os.path.join(train_dir, profile))\n",
    "            for name in names:\n",
    "                if name.startswith('.'):\n",
    "                    continue\n",
    "\n",
    "                if name.startswith('mask'):\n",
    "                    mask = 0\n",
    "                elif name.startswith('in'):\n",
    "                    mask = 1\n",
    "                else:\n",
    "                    mask = 2\n",
    "                name_path = os.path.join(train_dir, profile, name)\n",
    "                indices.append((name_path, gender, age, mask))\n",
    "        ## profiles_df\n",
    "        # gender: male -> 0, female -> 1\n",
    "        # mask: mask -> 0, incorrect -> 1, normal -> 2\n",
    "        profiles_df = pd.DataFrame(indices, columns=['path', 'gender', 'age', 'mask'])\n",
    "        self.X = profiles_df.path\n",
    "        self.Y = profiles_df.gender\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.X[index])\n",
    "        if self.transform:\n",
    "            if isinstance(self.transform, transforms.Compose):\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = self.transform(image=np.array(image))['image']\n",
    "            \n",
    "        return image, int(self.Y[index])\n",
    "    \n",
    "    def split_dataset(self) -> Tuple[Subset, Subset]:\n",
    "        n_val = int(len(self.X) * 0.2)\n",
    "        n_train = len(self.X) - n_val\n",
    "        train_set, val_set = random_split(self, [n_train, n_val])\n",
    "        return train_set, val_set\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932fa84",
   "metadata": {},
   "source": [
    "### mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4fac4642",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskDataset(Dataset):\n",
    "    def __init__(self, train_path, transform):\n",
    "        self.transform = transform\n",
    "        profiles = os.listdir(train_dir)\n",
    "        indices = []\n",
    "        for profile in profiles:\n",
    "            if profile.startswith('.'):\n",
    "                continue\n",
    "\n",
    "            id, gender, race, age = profile.split('_')\n",
    "            gender = 0 if gender == 'male' else 1\n",
    "\n",
    "            names = os.listdir(os.path.join(train_dir, profile))\n",
    "            for name in names:\n",
    "                if name.startswith('.'):\n",
    "                    continue\n",
    "\n",
    "                if name.startswith('mask'):\n",
    "                    mask = 0\n",
    "                elif name.startswith('in'):\n",
    "                    mask = 1\n",
    "                else:\n",
    "                    mask = 2\n",
    "                name_path = os.path.join(train_dir, profile, name)\n",
    "                indices.append((name_path, gender, age, mask))\n",
    "        ## profiles_df\n",
    "        # gender: male -> 0, female -> 1\n",
    "        # mask: mask -> 0, incorrect -> 1, normal -> 2\n",
    "        profiles_df = pd.DataFrame(indices, columns=['path', 'gender', 'age', 'mask'])\n",
    "        self.X = profiles_df.path\n",
    "        self.Y = profiles_df.mask\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.X[index])\n",
    "        if self.transform:\n",
    "            if isinstance(self.transform, transforms.Compose):\n",
    "                image = self.transform(image)\n",
    "            else:\n",
    "                image = self.transform(image=np.array(image))['image']\n",
    "            \n",
    "        return image, int(self.Y[index])\n",
    "    \n",
    "    def split_dataset(self) -> Tuple[Subset, Subset]:\n",
    "        n_val = int(len(self.X) * 0.2)\n",
    "        n_train = len(self.X) - n_val\n",
    "        train_set, val_set = random_split(self, [n_train, n_val])\n",
    "        return train_set, val_set\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a50e90-3bfd-4a12-bbff-8ca07cab69e0",
   "metadata": {},
   "source": [
    "### transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7f3dd386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --image_size\n",
    "image_size = (512, 384)\n",
    "\n",
    "# --transforms\n",
    "transform_train = albumentations.Compose([\n",
    "    albumentations.Resize(height=image_size[0], width=image_size[1], always_apply=True),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.HorizontalFlip(p=0.9),  # 좌우 반전\n",
    "        albumentations.VerticalFlip(p=0.9),  # 상하 반전,\n",
    "        albumentations.Affine(p=0.9),\n",
    "        albumentations.ShiftScaleRotate(\n",
    "            shift_limit=0.2,\n",
    "            scale_limit=0.2,\n",
    "            rotate_limit=10,\n",
    "            border_mode=0,\n",
    "            p=0.9),\n",
    "    ], p=0),  # p=0, for cutmix\n",
    "    albumentations.GaussNoise(p=0.4),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.MotionBlur(p=0.9),\n",
    "        albumentations.MedianBlur(blur_limit=3, p=0.9),\n",
    "        albumentations.Blur(blur_limit=3, p=0.9),\n",
    "    ], p=1),\n",
    "    albumentations.OneOf([\n",
    "        albumentations.HueSaturationValue(p=0.9),\n",
    "        albumentations.RGBShift(p=0.9),\n",
    "        albumentations.ChannelShuffle(p=0.9),\n",
    "        albumentations.ColorJitter(p=0.9),\n",
    "    ], p=1),\n",
    "\n",
    "    albumentations.CoarseDropout(max_holes=4,max_height=30,max_width=30,p=0.2),\n",
    "    albumentations.RandomBrightnessContrast(p=0.4),\n",
    "\n",
    "    albumentations.Normalize(mean=(0.5,0.5,0.5), std=(0.2,0.2,0.2)), \n",
    "    albumentations.pytorch.transforms.ToTensorV2(p=1)\n",
    "    ], p=1)\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    Resize(image_size, Image.BILINEAR),\n",
    "    ToTensor(),\n",
    "    Normalize(mean=(0.5, 0.5, 0.5), std=(0.2, 0.2, 0.2)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continued-feelings",
   "metadata": {},
   "source": [
    "## 2. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7820a8ce-fa4f-441c-a1d3-01cc763dfd08",
   "metadata": {},
   "source": [
    "### train setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7c388e65-ff5d-4bb0-8b2f-28bfda76df10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "627e3e98-35c4-4c7c-9699-cdf9aea873e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find bbox for cutmix\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]  # batch, channel, width, height\n",
    "    H = size[3]  # batch, channel, width, height\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (size[2] * size[3]))\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2, lam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "714d8abc-75d4-462a-9f6a-5f642da247f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --basic settings\n",
    "train_dir = '/opt/ml/input/data/train/images_augmented_1000'\n",
    "seed_everything(42)  # seed: 42\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "# --dataset & augmentation\n",
    "age_dataset = AgeDataset(train_dir,transform_train)\n",
    "age_train, age_val = age_dataset.split_dataset()\n",
    "\n",
    "gender_dataset = GenderDataset(train_dir,transform_train)\n",
    "gender_train, gender_val = age_dataset.split_dataset()\n",
    "\n",
    "mask_dataset = MaskDataset(train_dir,transform_train)\n",
    "mask_train, mask_val = age_dataset.split_dataset()\n",
    "\n",
    "# --dataloader\n",
    "age_train_loader = torch.utils.data.DataLoader(age_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "age_val_loader = torch.utils.data.DataLoader(age_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "gender_train_loader = torch.utils.data.DataLoader(gender_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "gender_val_loader = torch.utils.data.DataLoader(gender_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "mask_train_loader = torch.utils.data.DataLoader(mask_train, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "mask_val_loader = torch.utils.data.DataLoader(mask_val, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "# --load models\n",
    "age_model = models.resnet18(pretrained=True)\n",
    "gender_model = models.resnet18(pretrained=True)\n",
    "mask_model = models.resnet18(pretrained=True)\n",
    "\n",
    "# --initialize models\n",
    "age_model.fc = torch.nn.Linear(in_features=512, out_features=70, bias=True)\n",
    "torch.nn.init.kaiming_normal_(age_model.fc.weight)\n",
    "age_model.fc.bias.data.fill_(0.)\n",
    "\n",
    "gender_model.fc = torch.nn.Linear(in_features=512, out_features=2, bias=True)\n",
    "torch.nn.init.kaiming_normal_(gender_model.fc.weight)\n",
    "gender_model.fc.bias.data.fill_(0.)\n",
    "\n",
    "mask_model.fc = torch.nn.Linear(in_features=512, out_features=3, bias=True)\n",
    "torch.nn.init.kaiming_normal_(mask_model.fc.weight)\n",
    "mask_model.fc.bias.data.fill_(0.);\n",
    "\n",
    "age_model.to(device)\n",
    "gender_model.to(device)\n",
    "mask_model.to(device)\n",
    "\n",
    "# --loss & metric\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "LEARNING_RATE = 0.0001\n",
    "optim_age = torch.optim.Adam(age_model.parameters(), lr=LEARNING_RATE)\n",
    "optim_gender = torch.optim.Adam(gender_model.parameters(), lr=LEARNING_RATE)\n",
    "optim_mask = torch.optim.Adam(mask_model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# --train setting\n",
    "NUM_EPOCH = 10\n",
    "BATCH_SIZE = 64\n",
    "save_dir = os.path.join('/opt/ml/code/baselinecode_v1/model', '10_ensemble')\n",
    "log_interval = 300\n",
    "patience = 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "fbc75c2e-ecf0-420c-b4a6-542497838372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "해당 폴더가 이미 존재합니다.\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(save_dir):\n",
    "    print('해당 폴더가 이미 존재합니다.')\n",
    "else:\n",
    "    os.makedirs(save_dir)\n",
    "    print('폴더 생성 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dadc80-011f-41a4-8003-ec3b155113d4",
   "metadata": {},
   "source": [
    "### age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd34030d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train loop start !]\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "counter = 0\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # train loop\n",
    "    age_model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    cutmix_prob = 0.5  # cutmix 확률을 조절합니다. 0으로 하면, 실행하지 않습니다.\n",
    "    print('[train loop start !]')\n",
    "    for idx, train_batch in enumerate(age_train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optim_age.zero_grad()\n",
    "\n",
    "        ## cutmix part start ## \n",
    "        r = np.random.rand(1)\n",
    "        if np.random.rand(1) < cutmix_prob:\n",
    "            lam = np.random.beta(1.0, 1.0)  # 베타분포는 알파베타가 1이면, uniform분포가 됨\n",
    "            rand_index = torch.randperm(inputs.size()[0]).to(device)\n",
    "            label_a = labels\n",
    "            label_b = labels[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2, lam = rand_bbox(inputs.size(), lam) \n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            outs = age_model(inputs)\n",
    "            loss = lam * loss_fn(outs, label_a) + (1. - lam) * loss_fn(outs, label_b)  # mix_target\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "        ## cutmix part done ##\n",
    "        else:\n",
    "            outs = age_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = loss_fn(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optim_age.step()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % log_interval == 0:\n",
    "            train_loss = loss_value / log_interval\n",
    "            train_acc = matches / BATCH_SIZE / log_interval\n",
    "            current_lr = LEARNING_RATE\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{NUM_EPOCH}]({idx + 1}/{len(age_train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "    \n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"[Calculating validation results...]\")\n",
    "        age_model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        figure = None\n",
    "        for val_batch in age_val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = age_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = loss_fn(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(age_val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(age_val)\n",
    "        best_val_loss = min(best_val_loss, val_loss)\n",
    "        if val_acc > best_val_acc:\n",
    "            print(f\"New best model for val accuracy : {val_acc:4.2%}! saving the best model..\")\n",
    "            torch.save(age_model.state_dict(), f\"{save_dir}/age_best.pth\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # early stopping\n",
    "        if counter > patience:\n",
    "            print('Early Stopping...')\n",
    "            break\n",
    "\n",
    "        torch.save(age_model.state_dict(), f\"{save_dir}/age_last.pth\")\n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120ea911-4a2b-426a-a816-ff7c433d05f3",
   "metadata": {},
   "source": [
    "### gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60280c7-f276-4b0e-a54f-d15a4107c7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.\n",
    "best_val_loss = np.inf\n",
    "counter = 0\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # train loop\n",
    "    gemder_model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    cutmix_prob = 0.5  # cutmix 확률을 조절합니다. 0으로 하면, 실행하지 않습니다.\n",
    "    print('[train loop start !]')\n",
    "    for idx, train_batch in enumerate(gender_train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optim_gender.zero_grad()\n",
    "\n",
    "        ## cutmix part start ## \n",
    "        r = np.random.rand(1)\n",
    "        if np.random.rand(1) < cutmix_prob:\n",
    "            lam = np.random.beta(1.0, 1.0)  # 베타분포는 알파베타가 1이면, uniform분포가 됨\n",
    "            rand_index = torch.randperm(inputs.size()[0]).to(device)\n",
    "            label_a = labels\n",
    "            label_b = labels[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2, lam = rand_bbox(inputs.size(), lam) \n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            outs = gender_model(inputs)\n",
    "            loss = lam * loss_fn(outs, label_a) + (1. - lam) * loss_fn(outs, label_b)  # mix_target\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "        ## cutmix part done ##\n",
    "        else:\n",
    "            outs = gender_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = loss_fn(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optim_gender.step()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % log_interval == 0:\n",
    "            train_loss = loss_value / log_interval\n",
    "            train_acc = matches / BATCH_SIZE / log_interval\n",
    "            current_lr = LEARNING_RATE\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{NUM_EPOCH}]({idx + 1}/{len(gender_train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "        \n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"[Calculating validation results...]\")\n",
    "        gender_model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        figure = None\n",
    "        for val_batch in gender_val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = gender_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = loss_fn(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(gender_val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(gender_val)\n",
    "        best_val_loss = min(best_val_loss, val_loss)\n",
    "        if val_acc > best_val_acc:\n",
    "            print(f\"New best model for val accuracy : {val_acc:4.2%}! saving the best model..\")\n",
    "            torch.save(gender_model.state_dict(), f\"{save_dir}/gender_best.pth\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # early stopping\n",
    "        if counter > patience:\n",
    "            print('Early Stopping...')\n",
    "            break\n",
    "\n",
    "        torch.save(gender_model.state_dict(), f\"{save_dir}/gender_last.pth\")\n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )\n",
    "        print()\n",
    "        print('Calculating validation done !')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "753ab54b-7352-416e-95a2-f49fe2f1b787",
   "metadata": {},
   "source": [
    "### mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3003824",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "best_val_loss = np.inf\n",
    "counter = 0\n",
    "for epoch in range(NUM_EPOCH):\n",
    "    # train loop\n",
    "    gemder_model.train()\n",
    "    loss_value = 0\n",
    "    matches = 0\n",
    "    cutmix_prob = 0.5  # cutmix 확률을 조절합니다. 0으로 하면, 실행하지 않습니다.\n",
    "    print('[train loop start !]')\n",
    "    for idx, train_batch in enumerate(mask_train_loader):\n",
    "        inputs, labels = train_batch\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optim_mask.zero_grad()\n",
    "\n",
    "        ## cutmix part start ## \n",
    "        r = np.random.rand(1)\n",
    "        if np.random.rand(1) < cutmix_prob:\n",
    "            lam = np.random.beta(1.0, 1.0)  # 베타분포는 알파베타가 1이면, uniform분포가 됨\n",
    "            rand_index = torch.randperm(inputs.size()[0]).to(device)\n",
    "            label_a = labels\n",
    "            label_b = labels[rand_index]\n",
    "            bbx1, bby1, bbx2, bby2, lam = rand_bbox(inputs.size(), lam) \n",
    "            inputs[:, :, bbx1:bbx2, bby1:bby2] = inputs[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "            outs = mask_model(inputs)\n",
    "            loss = lam * loss_fn(outs, label_a) + (1. - lam) * loss_fn(outs, label_b)  # mix_target\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "        ## cutmix part done ##\n",
    "        else:\n",
    "            outs = mask_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "            loss = loss_fn(outs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optim_mask.step()\n",
    "\n",
    "        loss_value += loss.item()\n",
    "        matches += (preds == labels).sum().item()\n",
    "        if (idx + 1) % log_interval == 0:\n",
    "            train_loss = loss_value / log_interval\n",
    "            train_acc = matches / BATCH_SIZE / log_interval\n",
    "            current_lr = LEARNING_RATE\n",
    "            print(\n",
    "                f\"Epoch[{epoch}/{NUM_EPOCH}]({idx + 1}/{len(mask_train_loader)}) || \"\n",
    "                f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n",
    "            )\n",
    "\n",
    "            loss_value = 0\n",
    "            matches = 0\n",
    "        \n",
    "    # val loop\n",
    "    with torch.no_grad():\n",
    "        print(\"[Calculating validation results...]\")\n",
    "        mask_model.eval()\n",
    "        val_loss_items = []\n",
    "        val_acc_items = []\n",
    "        figure = None\n",
    "        for val_batch in mask_val_loader:\n",
    "            inputs, labels = val_batch\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outs = mask_model(inputs)\n",
    "            preds = torch.argmax(outs, dim=-1)\n",
    "\n",
    "            loss_item = loss_fn(outs, labels).item()\n",
    "            acc_item = (labels == preds).sum().item()\n",
    "            val_loss_items.append(loss_item)\n",
    "            val_acc_items.append(acc_item)\n",
    "\n",
    "        val_loss = np.sum(val_loss_items) / len(mask_val_loader)\n",
    "        val_acc = np.sum(val_acc_items) / len(mask_val)\n",
    "        best_val_loss = min(best_val_loss, val_loss)\n",
    "        if val_acc > best_val_acc:\n",
    "            print(f\"New best model for val accuracy : {val_acc:4.2%}! saving the best model..\")\n",
    "            torch.save(mask_model.state_dict(), f\"{save_dir}/mask_best.pth\")\n",
    "            best_val_acc = val_acc\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "        # early stopping\n",
    "        if counter > patience:\n",
    "            print('Early Stopping...')\n",
    "            break\n",
    "\n",
    "        torch.save(mask_model.state_dict(), f\"{save_dir}/mask_last.pth\")\n",
    "        print(\n",
    "            f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n",
    "            f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n",
    "        )\n",
    "        print()\n",
    "        print('Calculating validation done !')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db477eb",
   "metadata": {},
   "source": [
    "## 3.Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fdbfd4-c625-4ff0-a3ec-7084dbdcf26e",
   "metadata": {},
   "source": [
    "### test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c06ae05-b519-4b88-a3a6-d6e9f2421057",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_paths, transform):\n",
    "        self.img_paths = img_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.img_paths[index])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb9ad87-598b-46c0-a55d-2260964b24e4",
   "metadata": {},
   "source": [
    "### inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1d4c82-8cd0-4fd6-a04c-27d276635e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = '/opt/ml/input/data/eval'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-shade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# meta 데이터와 이미지 경로를 불러옵니다.\n",
    "submission = pd.read_csv(os.path.join(test_dir, 'info.csv'))\n",
    "image_dir = os.path.join(test_dir, 'images')\n",
    "\n",
    "# Test Dataset 클래스 객체를 생성하고 DataLoader를 만듭니다.\n",
    "image_paths = [os.path.join(image_dir, img_id) for img_id in submission.ImageID]\n",
    "dataset = TestDataset(image_paths, transform_test)\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# model\n",
    "age_model.eval()\n",
    "gender_model.eval()\n",
    "mask_model.eval()\n",
    "\n",
    "\n",
    "# 모델이 테스트 데이터셋을 예측하고 결과를 저장합니다.\n",
    "preds_age = []\n",
    "preds_gender = []\n",
    "preds_mask = []\n",
    "for images in loader:\n",
    "    with torch.no_grad():\n",
    "        images = images.to(device)\n",
    "        pred = age_model(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        preds_age.extend(pred.cpu().numpy())\n",
    "\n",
    "        pred = preds_gender(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        preds_gender.extend(pred.cpu().numpy())\n",
    "\n",
    "        pred = preds_mask(images)\n",
    "        pred = pred.argmax(dim=-1)\n",
    "        preds_mask.extend(pred.cpu().numpy())\n",
    "\n",
    "values = list(zip(preds_age,preds_gender,preds_mask))\n",
    "answers = []\n",
    "for value in values:\n",
    "    print(value)\n",
    "    age, gender, mask = value\n",
    "    if age < 30 :\n",
    "            c = 0\n",
    "    elif 30 <= age < 60:\n",
    "        c = 1\n",
    "    else:\n",
    "        c = 2\n",
    "        \n",
    "    if gender == 1:\n",
    "        c += 3\n",
    "    if mask == 1:\n",
    "        c += 6\n",
    "    elif mask == 2:\n",
    "        c += 12\n",
    "    #print(f'age : {age}, gender: {gender}, mask : {mask}, class : {c}')\n",
    "    answers.append(c)\n",
    "submission['ans'] = answers\n",
    "\n",
    "# 제출할 파일을 저장합니다.\n",
    "submission.to_csv(os.path.join(''/opt/ml/code/baselinecode_v1/output', 'output.csv'), index=False)\n",
    "print('test inference is done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ea6457",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-sample",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
